{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import nltk and newspaper, which will be used to tokenize, clean, and lemmatize the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T07:36:10.860866Z",
     "start_time": "2018-06-18T07:36:08.891604Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from newspaper import Article\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the newspaper library to parse the text from the article's URL:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T07:36:10.869497Z",
     "start_time": "2018-06-18T07:36:10.864625Z"
    }
   },
   "outputs": [],
   "source": [
    "def parseArticle(url):\n",
    "    article= Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    text = article.text\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use NLTK to tokenize, remove stop words, and lemmatize the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T07:36:11.175478Z",
     "start_time": "2018-06-18T07:36:11.163316Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessText(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    sr = stopwords.words('english')\n",
    "    for token in tokens:\n",
    "        if token in sr:\n",
    "            tokens.remove(token)\n",
    "        elif token in ['â€™', '\"','.', \"!\", \"?\", \",\"]:\n",
    "            tokens.remove(token)\n",
    "            \n",
    "    lemmatizer= WordNetLemmatizer()\n",
    "    lemmatized=[]\n",
    "    for token in tokens:\n",
    "        lemmatized.append(lemmatizer.lemmatize(token))\n",
    "    return(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extractEmotion(url) takes a url string as an argument and calls the helper functions parseArticle(url) and preprocessText(text), defined above. It takes a tally of the words in the article that are associated with each emotion in the NRC Emotion Lexicon. To get the emotion score, the function divides the tally by the total number of words in the article that were found in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T07:36:12.479240Z",
     "start_time": "2018-06-18T07:36:12.465332Z"
    }
   },
   "outputs": [],
   "source": [
    "def extractEmotion(url):\n",
    "    \n",
    "    #store data from emotion lexicon into a nested dictionary\n",
    "    emoLex= open('NRC_emotion_lexicon_list.txt', 'r')\n",
    "    emoDict= {}\n",
    "    for line in emoLex:\n",
    "        buffer=line.split()\n",
    "        if buffer[0] in emoDict:\n",
    "            emoDict[buffer[0]].update({buffer[1]:buffer[2]})\n",
    "        else:\n",
    "            emoDict[buffer[0]]={buffer[1]:buffer[2]}\n",
    "    emoLex.close()\n",
    "\n",
    "    text= parseArticle(url)\n",
    "    lemmatized= preprocessText(text)\n",
    "\n",
    "    anger= 0\n",
    "    anticipation= 0\n",
    "    joy= 0\n",
    "    sadness= 0\n",
    "    fear= 0\n",
    "    disgust= 0\n",
    "    trust= 0\n",
    "    surprise=0\n",
    "    positive=0\n",
    "    negative=0\n",
    "    total=0\n",
    "    found=0\n",
    "\n",
    "    for token in lemmatized:\n",
    "        total+=1\n",
    "        if token in emoDict:\n",
    "            found+=1\n",
    "            anger+= int(emoDict[token]['anger'])\n",
    "            anticipation+= int(emoDict[token]['anticipation'])\n",
    "            joy+= int(emoDict[token]['joy'])\n",
    "            sadness+= int(emoDict[token]['sadness'])\n",
    "            fear+= int(emoDict[token]['fear'])\n",
    "            disgust+= int(emoDict[token]['disgust'])\n",
    "            trust+= int(emoDict[token]['trust'])\n",
    "            surprise+= int(emoDict[token]['surprise'])\n",
    "            \n",
    "            positive+=int(emoDict[token]['positive'])\n",
    "            negative+=int(emoDict[token]['negative'])\n",
    "            \n",
    "    emotions=[anger/found, anticipation/found, joy/found, trust/found, fear/found, surprise/found, sadness/found, disgust/found]\n",
    "    return emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below runs extractEmotion(url) for each url in a list taken from the excel file provided. It writes the resulting emotion scores to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T08:12:39.804766Z",
     "start_time": "2018-06-18T08:11:19.354152Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shubharajan/anaconda3/envs/datasci/lib/python3.6/site-packages/dateutil/parser/_parser.py:1165: UnknownTimezoneWarning: tzname ET identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must `download()` an article first!\n"
     ]
    }
   ],
   "source": [
    "url_list=open(\"url_list.txt\", \"r\")\n",
    "output=open(\"testoutput.csv\",\"a\")\n",
    "for url in url_list:\n",
    "    printline=\"\\n\" + url.rstrip('\\n') + \",\"\n",
    "    try:\n",
    "        emotions=extractEmotion(url.rstrip('\\n'))\n",
    "    except:\n",
    "        output.write(printline)\n",
    "        continue\n",
    "    else:\n",
    "        for i in emotions:\n",
    "            printline+= str(i) + \",\"\n",
    "        output.write(printline)\n",
    "    \n",
    "url_list.close()\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T07:00:07.760371Z",
     "start_time": "2018-06-18T07:00:07.756859Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
